"""
LLM-based planner.

This module uses a real LLM to generate structured task plans from natural
language task descriptions.
"""

from __future__ import annotations

from dataclasses import asdict
from typing import Any, Dict, List, Optional

from .schemas import Subtask, TaskPlan, ToolName, validate_task_plan
from .state import TaskState
from .memory import Memory
from .llm import LLMClient, get_llm_client
from . import utils


class Planner:
    """
    Produce a structured multi-step plan for a natural language task.

    Public API:
        Planner.create_plan(task: str) -> TaskPlan

    The `plan` method is kept as a thin alias for backwards compatibility
    with earlier code in this project.
    """

    def __init__(self, llm_client: Optional[LLMClient] = None):
        """
        Initialize the planner.
        
        Parameters
        ----------
        llm_client: Optional[LLMClient]
            LLM client to use. If not provided, creates a default one.
        """
        self.llm = llm_client or get_llm_client()

    def create_plan(self, task: str, context: Optional[Dict[str, Any]] = None) -> TaskPlan:
        """
        Create a TaskPlan for the given high-level task description.

        This method:
        - Prompts an LLM with the task and a JSON schema
        - Generates multiple candidate plans
        - Selects the best plan
        - Validates the plan structure
        """
        utils.logger().info("Planner: creating plan", extra={"task": task})

        candidates = self._generate_candidate_plans(task, context=context, n=3)
        plan = self._select_best_plan(candidates, context=context)

        # Validate structure and constraints (5â€“15 subtasks, unique IDs, etc.)
        validate_task_plan(plan)
        return plan

    # Alias used by existing core loop
    def plan(self, task: str) -> TaskPlan:  # pragma: no cover - trivial wrapper
        return self.create_plan(task)

    # ------------------------------------------------------------------
    # LLM-based plan generation
    # ------------------------------------------------------------------

    def _generate_llm_plan(self, task: str, context: Optional[Dict[str, Any]] = None) -> List[Subtask]:
        """
        Use LLM to generate a plan with 5-15 subtasks for the given task.
        
        Parameters
        ----------
        task: str
            Task description
        context: Optional[Dict[str, Any]]
            Additional context (e.g., simplified task info)
        
        Returns
        -------
        List[Subtask]
            List of subtasks generated by the LLM
        """
        # Build the planning prompt
        context_str = ""
        if context:
            if "normalized_task" in context:
                context_str += f"\nNormalized task: {context['normalized_task']}\n"
            if "intents" in context:
                context_str += f"\nKey intents: {', '.join(context.get('intents', []))}\n"
        
        planning_prompt = f"""Break down the following task into 5-15 concrete, actionable subtasks.

Task: {task}{context_str}

For each subtask, provide:
- id: A unique identifier (e.g., "step-1", "step-2")
- description: Clear description of what needs to be done
- tool: One of ["generate_text", "search_in_files", "modify_data", "save_output"]
- dependencies: List of subtask IDs that must complete before this one (can be empty)
- success_criteria: How to know if this step succeeded
- deliverable: What output/artifact this step produces

Ensure:
- Subtasks are ordered logically
- Dependencies form a valid DAG (no circular dependencies)
- Each subtask is specific and actionable
- The plan covers the entire task from start to finish
- Include at least one step that saves the final output

Respond with a JSON object containing a "subtasks" array."""
        
        # Define the JSON schema
        schema = {
            "type": "object",
            "properties": {
                "subtasks": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {"type": "string"},
                            "description": {"type": "string"},
                            "tool": {
                                "type": "string",
                                "enum": ["generate_text", "search_in_files", "modify_data", "save_output"]
                            },
                            "dependencies": {
                                "type": "array",
                                "items": {"type": "string"}
                            },
                            "success_criteria": {"type": "string"},
                            "deliverable": {"type": "string"}
                        },
                        "required": ["id", "description", "tool", "dependencies", "success_criteria", "deliverable"]
                    }
                }
            },
            "required": ["subtasks"]
        }
        
        system_prompt = """You are an expert task planner. Break down complex tasks into clear, 
executable subtasks. Each subtask should be specific, measurable, and have clear success criteria."""
        
        try:
            # Generate plan using LLM
            response = self.llm.generate_json(
                planning_prompt,
                schema=schema,
                system_prompt=system_prompt,
            )
            
            # Parse response into Subtask objects
            subtasks = []
            for item in response.get("subtasks", []):
                try:
                    subtask = Subtask(
                        id=item["id"],
                        description=item["description"],
                        tool=ToolName(item["tool"]),
                        dependencies=item.get("dependencies", []),
                        success_criteria=item.get("success_criteria", ""),
                        deliverable=item.get("deliverable", ""),
                    )
                    subtasks.append(subtask)
                except (KeyError, ValueError) as e:
                    utils.logger().warning(
                        "Failed to parse subtask from LLM response",
                        extra={"item": item, "error": str(e)},
                    )
                    continue
            
            if not subtasks:
                raise ValueError("LLM did not generate any valid subtasks")
            
            utils.logger().debug(
                "Planner LLM output",
                extra={"subtasks": [asdict(s) for s in subtasks]},
            )
            return subtasks
            
        except Exception as e:
            utils.logger().error(
                "LLM plan generation failed, using fallback",
                extra={"error": str(e), "task": task},
            )
            # Fallback to a simple generic plan if LLM fails
            return self._fallback_plan(task)
    
    def _fallback_plan(self, task: str) -> List[Subtask]:
        """Generate a simple fallback plan if LLM fails."""
        return [
                Subtask(
                    id="step-1",
                description=f"Understand and clarify the task: {task}",
                    tool=ToolName.GENERATE_TEXT,
                    dependencies=[],
                success_criteria="Task requirements are clearly understood and documented.",
                deliverable="Task clarification document.",
                ),
                Subtask(
                    id="step-2",
                description="Break down the task into actionable steps.",
                    tool=ToolName.GENERATE_TEXT,
                    dependencies=["step-1"],
                success_criteria="List of actionable steps is generated.",
                deliverable="List of steps.",
                ),
                Subtask(
                    id="step-3",
                description="Search for any relevant existing information.",
                    tool=ToolName.SEARCH_IN_FILES,
                    dependencies=["step-2"],
                success_criteria="Search completed (results may be empty).",
                deliverable="Search results.",
                ),
                Subtask(
                    id="step-4",
                description="Execute the main work based on the plan.",
                    tool=ToolName.MODIFY_DATA,
                    dependencies=["step-2", "step-3"],
                success_criteria="Main work is completed.",
                deliverable="Completed work output.",
                ),
                Subtask(
                    id="step-5",
                description="Save the final output.",
                    tool=ToolName.SAVE_OUTPUT,
                    dependencies=["step-4"],
                success_criteria="Output is saved and can be retrieved.",
                deliverable="Storage key for saved output.",
            ),
        ]

    # ------------------------------------------------------------------
    # Tree/Graph-of-Thought helpers
    # ------------------------------------------------------------------

    def _generate_candidate_plans(
        self,
        task: str,
        context: Optional[Dict[str, Any]] = None,
        n: int = 3,
    ) -> List[TaskPlan]:
        """
        Generate N candidate TaskPlans for the given task using LLM.

        This uses the LLM to generate multiple plan variations, then selects the best one.
        """
        candidates: List[TaskPlan] = []

        # Generate multiple plans by asking the LLM to create variations
        for i in range(n):
            try:
                subtasks = self._generate_llm_plan(task, context=context)
                candidates.append(TaskPlan(task=task, subtasks=subtasks))
            except Exception as e:
                utils.logger().warning(
                    f"Failed to generate candidate plan {i+1}",
                    extra={"error": str(e)},
                )
                # Use fallback if LLM fails
                if i == 0:  # At least one fallback plan
                    candidates.append(TaskPlan(task=task, subtasks=self._fallback_plan(task)))
        
        # If we don't have enough candidates, duplicate the first one
        while len(candidates) < n:
            if candidates:
                candidates.append(candidates[0])
            else:
                candidates.append(TaskPlan(task=task, subtasks=self._fallback_plan(task)))
        
        return candidates[:n]  # Return exactly n candidates

    def _score_plan(
        self,
        plan: TaskPlan,
        context: Optional[Dict[str, Any]] = None,
    ) -> int:
        """
        Score a candidate plan using heuristics.

        Heuristics:
        - prefer plans that include both SEARCH_IN_FILES and SAVE_OUTPUT
        - prefer plans with 5-12 subtasks (good balance)
        - prefer plans with clear dependencies (well-structured)
        """
        tools = {s.tool for s in plan.subtasks}
        score = 0
        
        # Tool diversity bonus
        if ToolName.SEARCH_IN_FILES in tools:
            score += 2
        if ToolName.SAVE_OUTPUT in tools:
            score += 2
        if ToolName.MODIFY_DATA in tools:
            score += 1
        
        # Subtask count (prefer 5-12 range)
        num_subtasks = len(plan.subtasks)
        if 5 <= num_subtasks <= 12:
            score += 5
        elif num_subtasks < 5:
            score += num_subtasks  # Fewer points for too few
        else:
            score += max(0, 15 - num_subtasks)  # Penalize too many
        
        # Dependency structure bonus (plans with dependencies are better structured)
        has_dependencies = any(s.dependencies for s in plan.subtasks)
        if has_dependencies:
            score += 2
        
        return score

    def _select_best_plan(
        self,
        candidates: List[TaskPlan],
        context: Optional[Dict[str, Any]] = None,
    ) -> TaskPlan:
        """Pick the highest-scoring candidate plan."""
        if not candidates:
            raise ValueError("No candidate plans generated.")
        scored = [(self._score_plan(p, context=context), p) for p in candidates]
        best_score, best_plan = max(scored, key=lambda x: x[0])
        utils.logger().debug("Planner selected best plan", extra={"score": best_score})
        return best_plan

    # ------------------------------------------------------------------
    # Dynamic replanning
    # ------------------------------------------------------------------

    def replan(self, state: TaskState, memory: Memory) -> Optional[TaskPlan]:
        """
        Produce a small recovery plan when the current trajectory is failing.

        Uses LLM to analyze failures and generate recovery steps.
        """
        if not state.task_description:
            return None

        # Build context about what failed
        failed_results = [
            r for r in state.subtask_results
            if r.status.value == "failed"
        ]
        
        failure_summary = "No specific failures recorded."
        if failed_results:
            failure_descriptions = [
                f"Subtask {r.subtask_id}: {r.error or 'Unknown error'}"
                for r in failed_results[-3:]  # Last 3 failures
            ]
            failure_summary = "\n".join(failure_descriptions)
        
        # Get recent outputs for context
        recent_outputs = {}
        for subtask_id, output in list(memory.tool_outputs.items())[-5:]:
            recent_outputs[subtask_id] = str(output)[:200]  # Truncate long outputs
        
        replan_prompt = f"""The following task encountered failures and needs a recovery plan.

Original Task: {state.task_description}

Failures:
{failure_summary}

Recent Outputs:
{str(recent_outputs)[:500]}

Generate a recovery plan with 2-5 subtasks that:
1. Diagnose what went wrong
2. Address the root causes
3. Get the task back on track
4. Save the recovery recommendations

Respond with a JSON object containing a "subtasks" array (same format as planning)."""
        
        schema = {
            "type": "object",
            "properties": {
                "subtasks": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {"type": "string"},
                            "description": {"type": "string"},
                            "tool": {
                                "type": "string",
                                "enum": ["generate_text", "search_in_files", "modify_data", "save_output"]
                            },
                            "dependencies": {
                                "type": "array",
                                "items": {"type": "string"}
                            },
                            "success_criteria": {"type": "string"},
                            "deliverable": {"type": "string"}
                        },
                        "required": ["id", "description", "tool", "dependencies", "success_criteria", "deliverable"]
                    }
                }
            },
            "required": ["subtasks"]
        }
        
        system_prompt = """You are an expert at diagnosing task failures and creating recovery plans. 
Generate focused, actionable recovery steps that address the root causes of failures."""
        
        try:
            response = self.llm.generate_json(replan_prompt, schema=schema, system_prompt=system_prompt)
            
            subtasks = []
            for item in response.get("subtasks", []):
                try:
                    subtask = Subtask(
                        id=item["id"],
                        description=item["description"],
                        tool=ToolName(item["tool"]),
                        dependencies=item.get("dependencies", []),
                        success_criteria=item.get("success_criteria", ""),
                        deliverable=item.get("deliverable", ""),
                    )
                    subtasks.append(subtask)
                except (KeyError, ValueError) as e:
                    utils.logger().warning(
                        "Failed to parse recovery subtask",
                        extra={"item": item, "error": str(e)},
                    )
                    continue
            
            if subtasks:
                return TaskPlan(task=f"Recovery for: {state.task_description}", subtasks=subtasks)
        except Exception as e:
            utils.logger().error("LLM replanning failed", extra={"error": str(e)})
        
        # Fallback to simple recovery plan
        task = f"Recovery for: {state.task_description}"
        subtasks: List[Subtask] = [
            Subtask(
                id="replan-1",
                description="Analyse previous failures and missing information.",
                tool=ToolName.GENERATE_TEXT,
                dependencies=[],
                success_criteria="Summarises what went wrong and what is missing.",
                deliverable="Short diagnostic note.",
            ),
            Subtask(
                id="replan-2",
                description="Save updated recommendations to storage.",
                tool=ToolName.SAVE_OUTPUT,
                dependencies=["replan-1"],
                success_criteria="Recommendations are stored and referenced by a key.",
                deliverable="Storage key for updated recommendations.",
            ),
        ]
        return TaskPlan(task=task, subtasks=subtasks)


__all__ = ["Planner"]


